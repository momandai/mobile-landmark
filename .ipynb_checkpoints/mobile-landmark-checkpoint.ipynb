{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 587,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "import math\n",
    "import skimage\n",
    "import random\n",
    "from PIL import Image\n",
    "from skimage import io, transform\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, utils\n",
    "import xml.dom.minidom as xmldom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 595,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Ldk_300W_Dataset(Dataset):\n",
    "    def __init__(self, xmlfile, root_dir, transform=None):\n",
    "        document = xmldom.parse(xmlfile)\n",
    "        annos = document.getElementsByTagName(\"image\")\n",
    "        self.records = []\n",
    "        for anno in annos:\n",
    "            parts = anno.getElementsByTagName(\"part\")\n",
    "            landmark = []\n",
    "            for part in parts:\n",
    "                landmark.append(part.getAttribute(\"x\"))\n",
    "                landmark.append(part.getAttribute(\"y\"))\n",
    "            record = [anno.getAttribute(\"file\"), landmark]\n",
    "            self.records.append(record)\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "    def __len__(self):\n",
    "        return len(self.records)\n",
    "    def __getitem__(self, idx):\n",
    "        img_name = os.path.join(self.root_dir,\n",
    "                                self.records[idx][0])\n",
    "        image = io.imread(img_name)\n",
    "        h,w = image.shape[:2]\n",
    "        if len(image.shape) == 2:\n",
    "            image = image.reshape(h, w, 1)\n",
    "            image = np.concatenate((image,image,image),axis=2) \n",
    "        image = np.float32(image)/256\n",
    "        landmarks = map(float, self.records[idx][1])\n",
    "        landmarks = np.asarray(landmarks).reshape(-1, 2)\n",
    "        sample = {'image': image, 'landmarks': landmarks}\n",
    "        if self.transform:\n",
    "            sample = self.transform(sample)\n",
    "        return sample\n",
    "    def show(self,idx):\n",
    "        record = self[idx]\n",
    "        plt.scatter(record['landmarks'][:,0], record['landmarks'][:,1], s=10, marker='.', c='r')\n",
    "        plt.imshow(record['image'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 596,
   "metadata": {},
   "outputs": [],
   "source": [
    "#transform\n",
    "class CropByLDK(object):\n",
    "    def __init__(self, scale):\n",
    "        assert isinstance(scale, float)\n",
    "        if isinstance(scale, float):\n",
    "            self.scale = scale\n",
    "        else:\n",
    "            self.scale = 1.4\n",
    "            \n",
    "    def __call__(self, sample):\n",
    "        image, landmarks = sample['image'], sample['landmarks']\n",
    "        h, w = image.shape[:2]\n",
    "        left, right = landmarks[:,0].min(), landmarks[:,0].max()\n",
    "        top, bottom = landmarks[:,1].min(), landmarks[:,1].max()\n",
    "        height, width = bottom - top, right - left\n",
    "        l = int(max(height, width) * self.scale)\n",
    "\n",
    "        if left < 0 or right > w or top < 0 or bottom > h:\n",
    "            blank = np.zeros((l,l,3), dtype=np.float32)\n",
    "            blank[l/2 - height/2 : l/2 + height/2, l/2 - width/2 : l/2 + width/2] = image[top:bottom, left:right]\n",
    "            \n",
    "        middle = [left/2 + right/2, top/2 + bottom/2]\n",
    "        left, right = int(middle[0] - l/2), int(middle[0] + l/2)\n",
    "        top, bottom = int(middle[1] - l/2), int(middle[1] + l/2)            \n",
    "        image = image[top:bottom, left:right]\n",
    "        landmarks = landmarks - [left, top]\n",
    "        return {'image': image, 'landmarks':landmarks}\n",
    "    \n",
    "class Rescale(object):\n",
    "    def __init__(self, size):\n",
    "        assert isinstance(size, tuple)\n",
    "        self.size = size\n",
    "    def __call__(self, sample):\n",
    "        image, landmarks = sample['image'], sample['landmarks']\n",
    "        h, w = image.shape[:2]\n",
    "        image = transform.resize(image, (self.size[0], self.size[1]))\n",
    "        landmarks = landmarks * [self.size[0], self.size[1]] / [w, h]\n",
    "        return {'image': image, 'landmarks':landmarks}\n",
    "    \n",
    "class Rotate(object):\n",
    "    def __init__(self, angle):\n",
    "        self.angle = angle\n",
    "    def __call__(self, sample):\n",
    "        image, landmarks = sample['image'], sample['landmarks']\n",
    "        image = transform.rotate(image, self.angle)\n",
    "        left, right = min(landmarks[:,0]), max(landmarks[:,0])\n",
    "        top, bottom = min(landmarks[:,1]), max(landmarks[:,1])\n",
    "        angle = math.radians(-self.angle)\n",
    "        rotate_matrix = [[math.cos(angle), -math.sin(angle)]\n",
    "                          ,[math.sin(angle), math.cos(angle)]]\n",
    "        middle = [left/2 + right/2, top/2 + bottom/2]\n",
    "        landmarks_r = (landmarks - middle).transpose((1,0))\n",
    "        landmarks_r = np.matmul(rotate_matrix, landmarks_r)\n",
    "        landmarks_r = landmarks_r.transpose((1,0))\n",
    "        landmarks = landmarks_r + middle\n",
    "        \n",
    "        return {'image': image, 'landmarks': landmarks}\n",
    "    \n",
    "class Flip(object):\n",
    "    def __call__(self, sample):\n",
    "        image, landmarks = sample['image'], sample['landmarks']\n",
    "        h, w = image.shape[:2]\n",
    "        image = Image.fromarray(np.uint8(image * 256))\n",
    "        image = image.transpose(Image.FLIP_LEFT_RIGHT)\n",
    "        image = np.asarray(image)\n",
    "        image = np.float64(image)/256\n",
    "        landmarks[:, 0] = w - landmarks[:, 0]\n",
    "        return {'image' : image, 'landmarks': landmarks}\n",
    "\n",
    "class RandomCrop(object):\n",
    "    def __call__(self, sample):\n",
    "        image, landmarks = sample['image'], sample['landmarks']\n",
    "        r = image.shape[0]\n",
    "        size = int(r * random.uniform(0.9, 1.0))\n",
    "        init = r - size\n",
    "        x0 = random.randint(0, init - 1)\n",
    "        y0 = random.randint(0, init - 1)\n",
    "        x1 = x0 + int(size - 1)\n",
    "        y1 = y0 + int(size - 1)\n",
    "        print x0,y0,x1,y1, r\n",
    "        image = image[y0:y1, x0:x1]\n",
    "        landmarks = landmarks - [x0, y0]\n",
    "        return {'image' : image, 'landmarks': landmarks}\n",
    "        \n",
    "class Gnoise(object):\n",
    "    def __call__(self, sample):\n",
    "        image, landmarks = sample['image'], sample['landmarks']\n",
    "        image = skimage.util.random_noise(image,mode='gaussian',seed=None,clip=True)\n",
    "        return {'image': image, 'landmarks': landmarks}\n",
    "    \n",
    "class ToTensor(object):\n",
    "    def __call__(self, sample):\n",
    "        image, landmarks = sample['image'], sample['landmarks']\n",
    "        image = image.transpose((2, 0, 1))\n",
    "        return {'image': torch.from_numpy(image), 'landmarks': torch.from_numpy(landmarks)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 597,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_300w = '/home/whale/dataset/Landmark/300W'\n",
    "path_train = os.path.join(path_300w, \"train.xml\")\n",
    "path_test = os.path.join(path_300w, \"test.xml\")\n",
    "path_all = os.path.join(path_300w, \"all.xml\")\n",
    "composed = transforms.Compose([CropByLDK(1.4), Flip(), Rotate(30), RandomCrop(), Rescale((64,64)), ToTensor()])\n",
    "#inall = Ldk_300W_Dataset(path_all,path_300w)\n",
    "#train = Ldk_300W_Dataset(path_train, path_300w)\n",
    "test = Ldk_300W_Dataset(path_test, path_300w, composed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 598,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = DataLoader(test, batch_size=4, shuffle=True, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 602,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "out range\n",
      "out range\n",
      "out range\n",
      "8 14 264 270 273\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/whale/.local/lib/python2.7/site-packages/skimage/transform/_warps.py:105: UserWarning: The default mode, 'constant', will be changed to 'reflect' in skimage 0.15.\n",
      "  warn(\"The default mode, 'constant', will be changed to 'reflect' in \"\n",
      "/home/whale/.local/lib/python2.7/site-packages/skimage/transform/_warps.py:110: UserWarning: Anti-aliasing will be enabled by default in skimage 0.15 to avoid aliasing artifacts when down-sampling images.\n",
      "  warn(\"Anti-aliasing will be enabled by default in skimage 0.15 to \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 1 546 545 548\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/whale/.local/lib/python2.7/site-packages/skimage/transform/_warps.py:105: UserWarning: The default mode, 'constant', will be changed to 'reflect' in skimage 0.15.\n",
      "  warn(\"The default mode, 'constant', will be changed to 'reflect' in \"\n",
      "/home/whale/.local/lib/python2.7/site-packages/skimage/transform/_warps.py:110: UserWarning: Anti-aliasing will be enabled by default in skimage 0.15 to avoid aliasing artifacts when down-sampling images.\n",
      "  warn(\"Anti-aliasing will be enabled by default in skimage 0.15 to \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "out range\n",
      "2 1 694 693 698\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/whale/.local/lib/python2.7/site-packages/skimage/transform/_warps.py:105: UserWarning: The default mode, 'constant', will be changed to 'reflect' in skimage 0.15.\n",
      "  warn(\"The default mode, 'constant', will be changed to 'reflect' in \"\n",
      "/home/whale/.local/lib/python2.7/site-packages/skimage/transform/_warps.py:110: UserWarning: Anti-aliasing will be enabled by default in skimage 0.15 to avoid aliasing artifacts when down-sampling images.\n",
      "  warn(\"Anti-aliasing will be enabled by default in skimage 0.15 to \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "out range\n",
      "out range\n",
      "2 1 163 162 172\n",
      "8 7 468 467 480\n",
      "15 0 373 358 388\n",
      "2 11 1055 1064 1067\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/whale/.local/lib/python2.7/site-packages/skimage/transform/_warps.py:105: UserWarning: The default mode, 'constant', will be changed to 'reflect' in skimage 0.15.\n",
      "  warn(\"The default mode, 'constant', will be changed to 'reflect' in \"\n",
      "/home/whale/.local/lib/python2.7/site-packages/skimage/transform/_warps.py:110: UserWarning: Anti-aliasing will be enabled by default in skimage 0.15 to avoid aliasing artifacts when down-sampling images.\n",
      "  warn(\"Anti-aliasing will be enabled by default in skimage 0.15 to \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 4 223 226 228\n",
      "29 24 719 714 732\n",
      "out range\n",
      "4 4 214 214 228\n",
      "out range\n",
      "out range\n",
      "11 18 338 345 358\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Traceback (most recent call last):\n  File \"/home/whale/.local/lib/python2.7/site-packages/torch/utils/data/dataloader.py\", line 138, in _worker_loop\n    samples = collate_fn([dataset[i] for i in batch_indices])\n  File \"<ipython-input-595-b1b81851038a>\", line 31, in __getitem__\n    sample = self.transform(sample)\n  File \"/home/whale/.local/lib/python2.7/site-packages/torchvision/transforms/transforms.py\", line 49, in __call__\n    img = t(img)\n  File \"<ipython-input-596-dc402aeced47>\", line 62, in __call__\n    image = Image.fromarray(np.uint8(image * 256))\n  File \"/home/whale/.local/lib/python2.7/site-packages/PIL/Image.py\", line 2536, in fromarray\n    return frombuffer(mode, size, obj, \"raw\", rawmode, 0, 1)\n  File \"/home/whale/.local/lib/python2.7/site-packages/PIL/Image.py\", line 2479, in frombuffer\n    return frombytes(mode, size, data, decoder_name, args)\n  File \"/home/whale/.local/lib/python2.7/site-packages/PIL/Image.py\", line 2412, in frombytes\n    im.frombytes(data, decoder_name, args)\n  File \"/home/whale/.local/lib/python2.7/site-packages/PIL/Image.py\", line 811, in frombytes\n    d.setimage(self.im)\nValueError: tile cannot extend outside image\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-602-8ae4fc21b011>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtitle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Batch from dataloader'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mi_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_batched\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m     print(i_batch, sample_batched['image'].size(),\n\u001b[1;32m     19\u001b[0m           sample_batched['landmarks'].size())\n",
      "\u001b[0;32m/home/whale/.local/lib/python2.7/site-packages/torch/utils/data/dataloader.pyc\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    635\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreorder_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    636\u001b[0m                 \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 637\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_process_next_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    638\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    639\u001b[0m     \u001b[0mnext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m__next__\u001b[0m  \u001b[0;31m# Python 2 compatibility\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/whale/.local/lib/python2.7/site-packages/torch/utils/data/dataloader.pyc\u001b[0m in \u001b[0;36m_process_next_batch\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    656\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_put_indices\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    657\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mExceptionWrapper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 658\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexc_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexc_msg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    659\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    660\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Traceback (most recent call last):\n  File \"/home/whale/.local/lib/python2.7/site-packages/torch/utils/data/dataloader.py\", line 138, in _worker_loop\n    samples = collate_fn([dataset[i] for i in batch_indices])\n  File \"<ipython-input-595-b1b81851038a>\", line 31, in __getitem__\n    sample = self.transform(sample)\n  File \"/home/whale/.local/lib/python2.7/site-packages/torchvision/transforms/transforms.py\", line 49, in __call__\n    img = t(img)\n  File \"<ipython-input-596-dc402aeced47>\", line 62, in __call__\n    image = Image.fromarray(np.uint8(image * 256))\n  File \"/home/whale/.local/lib/python2.7/site-packages/PIL/Image.py\", line 2536, in fromarray\n    return frombuffer(mode, size, obj, \"raw\", rawmode, 0, 1)\n  File \"/home/whale/.local/lib/python2.7/site-packages/PIL/Image.py\", line 2479, in frombuffer\n    return frombytes(mode, size, data, decoder_name, args)\n  File \"/home/whale/.local/lib/python2.7/site-packages/PIL/Image.py\", line 2412, in frombytes\n    im.frombytes(data, decoder_name, args)\n  File \"/home/whale/.local/lib/python2.7/site-packages/PIL/Image.py\", line 811, in frombytes\n    d.setimage(self.im)\nValueError: tile cannot extend outside image\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 5 586 587 630\n",
      "6 9 408 411 418\n",
      "7 53 722 768 770\n",
      "0 3 443 446 458\n",
      "10 6 238 234 248\n",
      "40 52 559 571 574\n",
      "1 6 170 175 178\n",
      "10 7 312 309 324\n",
      "1 1 238 238 240\n",
      "49 58 729 738 740\n",
      "8 15 330 337 340\n",
      "2 1 258 257 266\n",
      "11 1 506 496 516\n"
     ]
    }
   ],
   "source": [
    "def show_landmarks_batch(sample_batched):\n",
    "    \"\"\"Show image with landmarks for a batch of samples.\"\"\"\n",
    "    images_batch, landmarks_batch = \\\n",
    "            sample_batched['image'], sample_batched['landmarks']\n",
    "    batch_size = len(images_batch)\n",
    "    im_size = images_batch.size(2)\n",
    "\n",
    "    grid = utils.make_grid(images_batch)\n",
    "    plt.imshow(grid.numpy().transpose((1, 2, 0)))\n",
    "\n",
    "    for i in range(batch_size):\n",
    "        plt.scatter(landmarks_batch[i, :, 0].numpy() + i * (im_size+2),\n",
    "                    landmarks_batch[i, :, 1].numpy(),\n",
    "                    s=10, marker='.', c='r')\n",
    "\n",
    "        plt.title('Batch from dataloader')\n",
    "\n",
    "for i_batch, sample_batched in enumerate(dataloader):\n",
    "    print(i_batch, sample_batched['image'].size(),\n",
    "          sample_batched['landmarks'].size())\n",
    "\n",
    "    # observe 4th batch and stop.\n",
    "    if i_batch == 0:\n",
    "        plt.figure()\n",
    "        show_landmarks_batch(sample_batched)\n",
    "        plt.axis('off')\n",
    "        plt.ioff()\n",
    "        plt.show()\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
